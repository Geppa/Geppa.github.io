---
layout: home
profile_picture:
  src: /assets/img/profile-pic.jpg
  alt: website picture
---
<p style="font-size: 18px;">
  <strong>Info</strong><br>
</p>
<p>
  Ph.D Student <br>at <a href="http://ailab.khu.ac.kr/" target="_blank">Augmented Intelligence(AI) Laboratory</a><br>from <a href="https://khu.ac.kr/">Kyung Hee University</a> <br>contact with <strong>asdjklfgh97 at khu.ac.kr </strong>
</p>
<p>
  <a href="/assets/pdf/CV_MinKukKim.pdf" target="_blank">CV</a> / <a href="https://github.com/Geppa" target="_blank">Github</a> / <a href="https://www.linkedin.com/in/minkuk-kim-71b5482bb/?locale=en_US" target="_blank">LinkedIn</a> / <a href="https://scholar.google.com/citations?user=omTinbUAAAAJ&hl=ko" target="_blank">Google Scholar</a><br>
</p>
<p>
  <strong>For any suggestion, please contact me with <a href="mailto:asdjklfgh97@khu.ac.kr">Email</a>.</strong><br>
</p>
<br>
<p style="font-size: 18px;">
  <strong>Short Bio</strong><br>
</p>
<p>
  My name is Minkuk Kim, and I am currently a Ph.D. student at <a href="https://khu.ac.kr/" target="_blank">Kyung Hee University</a>'s Department of Artificial Intelligence, where I am part of the <a href="http://amilab.khu.ac.kr/" target="_blank">Augmented Intelligence (AMI) Laboratory</a>. Under the supervision of Prof. <a href="https://sites.google.com/site/sseongtaekim/home?authuser=0" target="_blank">Seong-Tae Kim</a>, my research lies at the intersection of computer vision and natural language processing, with a particular focus on multi-modal learning and memory-augmented reasoning.

  <br><br>

  My recent work has explored how external memory and cross-modal retrieval can enhance dense video captioning. At CVPR 2024, I presented a novel framework that retrieves relevant textual cues from memory to improve both event localization and caption generation in untrimmed videos. At AAAI 2025, I further introduced a hierarchical compact memory structure inspired by human cognition, which organizes memory information across multiple levels of abstraction. This approach enables both improved semantic recall and efficient retrieval in long and complex video contexts. Through these studies, I aim to bridge episodic memory modeling with multi-modal sequence understanding in video-language tasks.

  <br><br>

  In the short term, I am particularly interested in advancing multi-modal AI systems to handle long-form, real-world video content—such as hour-long narratives—by integrating structured memory mechanisms with scalable temporal reasoning.

  <br><br>

  My goal is to continue advancing the field of AI by developing innovative solutions that bridge the gap between visual and textual data, ultimately contributing to the creation of intelligent systems that understand and interact with the world more naturally and effectively.
</p>
<br>
<p style="font-size: 18px;">
  <strong>Recent News</strong><br>
</p>
<p>
Dec. 2024 One paper on Vision-Language Models got accepted to AAAI 2025!<br>
Aug. 2024 Completed my MS program and start as PhD student!<br>
Jun. 2024 One paper got accepted to ICIP Workshop 2024!<br>
Feb. 2024 One paper on Vision-Language Models got accepted to CVPR 2024!<br>
Nov. 2023 One paper got accepted to Image and Vision Computing!<br>
<p>